{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "648d8d15-29b3-4d26-819f-7a027b07fb22",
   "metadata": {},
   "source": [
    "## Q1. Explain the properties of the F-distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3b30d0-0a5b-42e7-b1d0-6184da9c492a",
   "metadata": {},
   "source": [
    "The F-distribution curve is positively skewed towards the right.\n",
    "\n",
    "The value of F is always positive or zero. No negative values.\n",
    "\n",
    "The shape of the distribution depends on the degrees of freedom of numerator and denominator.\n",
    "\n",
    "The value of the F-distribution is always positive, or zero since the variances are the square of the deviations and hence cannot assume negative values.\n",
    "\n",
    "The F-distribution is not symmetrical but skewed to the right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0223ee-4edb-48e5-b6f8-0e7b8a3b6124",
   "metadata": {},
   "source": [
    "## Q2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871543f0-840e-40db-92cc-6624bf656cb3",
   "metadata": {},
   "source": [
    "The F-distribution is used in a variety of statistical tests, primarily those that involve comparing variances, assessing model fits, or evaluating the relationship between multiple group means. Here are the main types of statistical tests in which the F-distribution plays a critical role and the reasons it is appropriate for these tests:\n",
    "\n",
    "### 1. Analysis of Variance (ANOVA):\n",
    "Purpose: ANOVA is used to test whether there are significant differences between the means of multiple groups. For example, in a one-way ANOVA, you might compare the means of three or more groups to determine if at least one mean differs from the others.\n",
    "\n",
    "#### Why the F-distribution?\n",
    "ANOVA compares the variance between groups (which reflects how group means differ from the overall mean) with the variance within groups (which reflects how individual data points vary within each group).\n",
    "The ratio of these two variances follows an F-distribution under the null hypothesis, which assumes that all group means are equal (i.e., no treatment effect or group differences).\n",
    "\n",
    "F-statistic in ANOVA:\n",
    "\n",
    "ùêπ = Between-group¬†variance / Within-group¬†variance\n",
    "‚Äã\n",
    "The larger the ratio, the more likely it is that the group means are different.\n",
    "\n",
    "### 2. Two-Way ANOVA:\n",
    "Purpose: Two-way ANOVA extends the one-way ANOVA to examine the effect of two independent variables (factors) on a dependent variable, and it can also test for interaction between the factors.\n",
    "\n",
    "#### Why the F-distribution?\n",
    "Similar to one-way ANOVA, two-way ANOVA uses F-statistics to test hypotheses about the effect of the individual factors and their interaction.\n",
    "Each factor (main effect) and the interaction effect is associated with an F-statistic, and the distribution of these test statistics follows the F-distribution.\n",
    "\n",
    "### 3. Regression Analysis (Including Multiple Regression):\n",
    "Purpose: Regression analysis, particularly multiple linear regression, is used to model the relationship between a dependent variable and one or more independent variables.\n",
    "\n",
    "#### Why the F-distribution?\n",
    "In regression, the overall goodness-of-fit of the model is tested using an F-test. The F-statistic in regression tests whether the model as a whole provides a better fit to the data than a model with no predictors (i.e., just the intercept).\n",
    "The null hypothesis in this case is that all regression coefficients (except the intercept) are zero, implying that none of the independent variables have a significant relationship with the dependent variable.\n",
    "\n",
    "The F-statistic is computed as the ratio of the model‚Äôs explained variance to the unexplained variance, and this ratio follows an F-distribution under the null hypothesis.\n",
    "\n",
    "F-statistic in regression:\n",
    "\n",
    "ùêπ = Explained¬†variance¬†(model) / Unexplained¬†variance¬†(residuals)\n",
    "‚Äã\n",
    "### 4. Comparing Two Variances (F-Test for Equality of Variances):\n",
    "Purpose: The F-test for comparing two variances is used to determine whether two populations have the same variance.\n",
    "#### Why the F-distribution?\n",
    "The F-distribution is particularly suitable for this test because it is the distribution of the ratio of two sample variances, each scaled by their respective degrees of freedom.\n",
    "The null hypothesis in this test is that the variances of two populations are equal. If the ratio of the sample variances is much greater than 1 (or much smaller), it suggests that the variances are different.\n",
    "\n",
    "### 5. General Linear Models (GLMs):\n",
    "Purpose: GLMs include a wide range of statistical models that describe relationships between a dependent variable and one or more independent variables (including both continuous and categorical variables).\n",
    "#### Why the F-distribution?\n",
    "In the context of GLMs, the F-test is often used to assess the significance of one or more predictors in the model. It tests whether the model with the predictors fits significantly better than a baseline model.\n",
    "The F-test in GLMs compares the explained variance from the model with the unexplained variance (residual variance), which follows an F-distribution under the null hypothesis.\n",
    "\n",
    "### 6. Multivariate Analysis of Variance (MANOVA):\n",
    "Purpose: MANOVA is an extension of ANOVA that is used when there are multiple dependent variables. It assesses whether the mean vectors of different groups differ significantly.\n",
    "#### Why the F-distribution?\n",
    "Similar to ANOVA, MANOVA uses F-statistics to test hypotheses about group differences. It compares the variance between the groups for the multivariate data to the variance within the groups.\n",
    "The F-statistic in MANOVA is derived from the ratio of these variances and follows the F-distribution.\n",
    "\n",
    "## Why the F-distribution is Appropriate for These Tests:\n",
    "### Variance Ratios:\n",
    "The F-distribution is appropriate for tests comparing ratios of variances (e.g., between-group vs. within-group variance, or explained vs. unexplained variance). The F-test is essentially based on comparing the variability between different groups or factors, and the F-distribution describes the sampling distribution of these ratios.\n",
    "\n",
    "### Non-Normality of the Test Statistic:\n",
    "When comparing variances or testing for overall model fit, the F-statistic is derived from chi-squared distributions (which are non-normal) and is designed to handle this non-normality. The F-distribution accounts for the sampling behavior of variance estimates.\n",
    "\n",
    "### Positive Values:\n",
    "Since variances cannot be negative, the F-statistic will always be positive, making the F-distribution a natural choice for these tests, as it only defines positive values on the range \n",
    "\n",
    "As sample sizes increase, the F-distribution becomes more symmetric, and its shape becomes less skewed. This makes it a good approximation for the behavior of ratio-based test statistics as the number of observations grows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9283483a-7b3f-49f4-89b0-c1fc60d12ab3",
   "metadata": {},
   "source": [
    "## Q3. What are the key assumptions required for conducting an F-test to compare the variances of two populations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54b6cfe-c283-46fb-90b3-c7a90a2d090a",
   "metadata": {},
   "source": [
    "To conduct an F-test for comparing the variances of two populations, several key assumptions must be met for the test to be valid and produce reliable results. These assumptions ensure that the F-statistic follows the F-distribution under the null hypothesis and that the test provides accurate conclusions. Here are the key assumptions:\n",
    "#### 1. Independence of the Samples:\n",
    "The two samples being compared must be independent of each other. This means that the selection of observations for one sample must not influence the selection of observations for the other sample. The independence assumption is crucial because any dependence between the two samples can distort the calculation of the F-statistic and lead to incorrect conclusions.\n",
    "\n",
    "#### 2. Normality of the Populations:\n",
    "The populations from which the two samples are drawn should be approximately normally distributed. The F-test relies on the assumption that the sample variances are estimates of the variances of normal populations. If the populations are not normally distributed, the F-test may yield incorrect results, especially when the sample sizes are small.\n",
    "\n",
    "This assumption can be relaxed to some extent if the sample sizes are large enough, thanks to the Central Limit Theorem, which states that the sampling distribution of the sample variance approaches normality as the sample size increases.\n",
    "\n",
    "#### 3. Random Sampling:\n",
    "Both samples must be randomly selected from their respective populations. This ensures that the samples are representative of their populations and reduces the risk of bias. Non-random sampling could lead to skewed or unrepresentative sample variances, which would affect the validity of the test.\n",
    "\n",
    "#### 4. Homogeneity of Variances (Null Hypothesis):\n",
    "The null hypothesis of the F-test for comparing variances assumes that the two populations have the same variance.This is often referred to as the assumption of homogeneity of variances or homoscedasticity. The F-test specifically tests whether the ratio of the two sample variances deviates significantly from 1, which would indicate a difference in population variances.\n",
    "\n",
    "In practice, if the variances of the two populations are very different, the F-test might not perform well, especially if the sample sizes are small. In such cases, alternative tests (e.g., Welch‚Äôs test or the Levene test) may be used.\n",
    "\n",
    "#### 5. Scale of Measurement (Ratio or Interval):\n",
    "The data in each sample should be measured on at least an interval or ratio scale, meaning the data should be continuous and have meaningful numerical values with a consistent unit of measurement (e.g., weight, height, time, or temperature).\n",
    "\n",
    "This is important because the F-test compares the variances of the two samples, and variance is a measure of spread or dispersion that requires continuous data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2169957-85fe-4608-907f-878e2da6d0c3",
   "metadata": {},
   "source": [
    "## Q4.  What is the purpose of ANOVA, and how does it differ from a t-test? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe85202e-15fd-4f40-b521-43dd21d74b51",
   "metadata": {},
   "source": [
    "### Purpose of ANOVA (Analysis of Variance):\n",
    "ANOVA is a statistical method used to compare means across three or more groups (or levels) to determine whether there is a significant difference between them. The main goal of ANOVA is to test the hypothesis that the means of multiple groups are equal, based on sample data. It helps to assess whether the observed differences between group means are large enough to be considered statistically significant, or whether they are likely due to random chance.\n",
    "\n",
    "### Key Objectives of ANOVA:\n",
    "#### Test for Differences Between Group Means:\n",
    "ANOVA compares the variance between the groups (i.e., variability due to the group effect) with the variance within the groups (i.e., variability due to individual differences within each group).\n",
    "If the variance between groups is significantly greater than the variance within groups, we conclude that at least one group mean is different from the others.\n",
    "\n",
    "#### Analysis of Group Variation:\n",
    "ANOVA helps partition the total variation in the data into components attributable to different sources (e.g., between-group variance and within-group variance), making it easier to understand the causes of variability.\n",
    "\n",
    "### ANOVA Differs from a t-test:\n",
    "#### 1. Number of Groups Compared:\n",
    "ANOVA: Used to compare three or more group means.\n",
    "t-test: Typically used to compare the means of two groups only.\n",
    "\n",
    "#### 2. Testing Strategy:\n",
    "ANOVA: Tests the null hypothesis that all group means are equal. It does this by examining the ratio of between-group variance to within-group variance. If the ratio is large, it suggests that at least one group mean differs significantly from the others.\n",
    "\n",
    "t-test: Tests whether there is a significant difference between the means of two groups.\n",
    "\n",
    "#### 3. F-statistic vs. t-statistic:\n",
    "ANOVA: The test statistic is the F-statistic, which is the ratio of between-group variance to within-group variance.\n",
    "\n",
    "ùêπ = Variance¬†within¬†groups / Variance¬†between¬†groups\n",
    "‚Äã\n",
    "t-test: The test statistic is the t-statistic, which compares the difference between the sample means relative to the standard error of the difference.\n",
    "\n",
    "t = Difference¬†between¬†group¬†means / Standard¬†error¬†of¬†the¬†difference\n",
    " \n",
    "#### 4. Multiple Comparisons:\n",
    "ANOVA: If ANOVA indicates that there are significant differences between groups, post hoc tests (such as Tukey‚Äôs HSD, Bonferroni, etc.) are often conducted to determine which specific groups differ from each other. ANOVA does not tell you which means are different, just that at least one is different.\n",
    "\n",
    "t-test: The t-test directly compares the means of the two groups but can only assess a single pairwise comparison. If you want to compare more than \n",
    "two groups using t-tests, you must perform multiple pairwise t-tests, which increases the risk of Type I error (false positives).\n",
    "\n",
    "#### 5. Application to More Than Two Groups:\n",
    "ANOVA: Specifically designed for situations where there are more than two groups. Using multiple t-tests for more than two groups would require many pairwise comparisons, increasing the likelihood of Type I error.\n",
    "\n",
    "t-test: Limited to two groups. If you want to compare more than two groups, performing multiple t-tests would increase the chance of incorrectly rejecting the null hypothesis due to the cumulative error rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079abe9c-eb10-44a1-acad-e6796478983d",
   "metadata": {},
   "source": [
    "## Q5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36545aed-c6ac-44aa-a422-c97429060ca9",
   "metadata": {},
   "source": [
    "You should use one-way ANOVA instead of multiple t-tests when comparing the means of three or more groups for the following reasons:\n",
    "\n",
    "#### 1. Control Type I Error:\n",
    "Multiple t-tests increase the risk of committing a Type I error (false positives). Each t-test has a 5% chance of error, and conducting multiple tests inflates the overall error rate. ANOVA tests all group means at once, controlling the overall Type I error.\n",
    "\n",
    "#### 2. Efficiency:\n",
    "ANOVA is more efficient because it tests for differences among all groups in a single step, whereas multiple t-tests require several comparisons, increasing the risk of errors and computational complexity.\n",
    "\n",
    "#### 3. More Powerful:\n",
    "ANOVA uses all the data to estimate group differences, making it more statistically powerful than running several t-tests, which only focus on pairwise comparisons.\n",
    "\n",
    "#### 4. Assumption Testing:\n",
    "ANOVA allows you to check assumptions (e.g., normality and equal variances) for all groups simultaneously, while t-tests require separate checks for each pair.\n",
    "\n",
    "Example:\n",
    "If comparing three teaching methods, using ANOVA tests if any method significantly differs from the others, while multiple t-tests compare two methods at a time, risking inflated errors. If ANOVA is significant, you can follow up with post-hoc tests to pinpoint the differences.\n",
    "\n",
    "In short, use one-way ANOVA when comparing multiple groups to control Type I error, increase power, and simplify the process.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c44beb-9724-4bf9-8862-f7aadcc048c3",
   "metadata": {},
   "source": [
    "## Q6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning contribute to the calculation of the F-statistic?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568973d0-223f-4c8b-b938-30c288d2cabe",
   "metadata": {},
   "source": [
    "In ANOVA, variance is partitioned into two components:\n",
    "\n",
    "between-group variance and within-group variance.\n",
    "\n",
    "### 1. Total Variance (SST):\n",
    "Measures the overall variability of the data around the grand mean (average of all data points).\n",
    "\n",
    "### 2. Between-Group Variance (SSB):\n",
    "Measures how much the group means differ from the grand mean. Larger differences between group means increase the between-group variance.\n",
    "\n",
    "### 3. Within-Group Variance (SSE):\n",
    "Measures the variation within each group, reflecting the spread of individual data points around their respective group means.\n",
    "\n",
    "### F-statistic Calculation:\n",
    "The F-statistic is the ratio of between-group variance to within-group variance:\n",
    "\n",
    "F = MSB / MSW\n",
    "\n",
    "Where:\n",
    "- **MSB** = Mean Square Between (variance between groups)\n",
    "- **MSW** = Mean Square Within (variance within groups)\n",
    "\n",
    "### **Interpretation of F-statistic:**\n",
    "- A Large F-statistic (MSB > MSW) suggests significant differences between group means.\n",
    "- A Small F-statistic (MSB ‚âà MSW) suggests no significant difference, indicating that the group means are likely equal.\n",
    "\n",
    "In summary, the F-statistic tests whether the variation between the group means is significantly greater than the variation within the groups, helping to determine if there are meaningful differences among the group means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7bbe95-3ca9-4bb4-adb1-dd2431b7e47b",
   "metadata": {},
   "source": [
    "## Q7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f3c839-31da-41bf-886d-bb489dd90549",
   "metadata": {},
   "source": [
    "The classical (frequentist) approach to ANOVA and the Bayesian approach differ fundamentally in how they handle uncertainty, parameter estimation, and hypothesis testing. Here's a breakdown of the key differences:\n",
    "\n",
    "### 1. Handling of Uncertainty\n",
    "\n",
    "#### Frequentist Approach:\n",
    "\n",
    "In the frequentist framework, uncertainty is viewed as arising from variability in repeated sampling or experiments. The probability describes the long-run frequency of events in repeated sampling, and it is used to evaluate how consistent the observed data is with a given model.\n",
    "Parameters are considered fixed but unknown quantities, and uncertainty about the parameters is quantified through the data and sampling distributions. Confidence intervals, p-values, and test statistics reflect this uncertainty.\n",
    "\n",
    "#### Bayesian Approach:\n",
    "\n",
    "In the Bayesian framework, uncertainty is represented probabilistically and is viewed as subjective belief about the parameters, given prior knowledge and the data observed. Probabilities are interpreted as degrees of belief, and these beliefs can be updated using Bayes' theorem.\n",
    "Prior distributions are assigned to parameters, which encapsulate the researcher‚Äôs prior knowledge or beliefs. The uncertainty about the parameters is updated in light of the observed data, leading to a posterior distribution. This posterior distribution reflects both prior beliefs and data-derived information.\n",
    "\n",
    "### 2. Parameter Estimation\n",
    "\n",
    "#### Frequentist Approach:\n",
    "\n",
    "Parameters are estimated using methods such as Maximum Likelihood Estimation (MLE), where the goal is to find parameter values that maximize the likelihood of the observed data, given the model.\n",
    "Once the parameters are estimated, uncertainty about these parameters is typically quantified using standard errors, confidence intervals, or likelihood ratio tests. These are considered as estimates of the true, fixed values of the parameters, based on the data at hand.\n",
    "\n",
    "#### Bayesian Approach:\n",
    "\n",
    "Parameters are treated as random variables, and estimation is based on the posterior distribution, which combines prior beliefs (prior distribution) with the data (likelihood function). The point estimates can be derived from the posterior distribution (e.g., mean, median, or mode), but the key insight is that there is an entire distribution for each parameter, reflecting the uncertainty about its true value.\n",
    "In Bayesian ANOVA, the posterior distribution provides a more comprehensive view of parameter estimates, including uncertainty, rather than a single point estimate as in the frequentist approach.\n",
    "\n",
    "### 3. Hypothesis Testing\n",
    "\n",
    "#### Frequentist Approach:\n",
    "\n",
    "Hypothesis testing in the frequentist framework relies on p-values, test statistics (such as the F-statistic in ANOVA), and the null hypothesis. The null hypothesis is rejected if the p-value is below a chosen significance level (e.g., Œ± = 0.05).\n",
    "The p-value indicates the probability of observing data at least as extreme as the data observed, assuming the null hypothesis is true. A smaller p-value suggests stronger evidence against the null hypothesis.\n",
    "\n",
    "Hypothesis testing in the frequentist framework is often criticized for relying on arbitrary thresholds (such as p < 0.05) and not directly providing the probability of hypotheses themselves.\n",
    "\n",
    "#### Bayesian Approach:\n",
    "\n",
    "Bayesian hypothesis testing is based on comparing the posterior probabilities of different hypotheses, often using the Bayes factor, which is the ratio of the likelihood of the data under one hypothesis to the likelihood of the data under another hypothesis.\n",
    "A Bayes factor greater than 1 suggests that the data favor one hypothesis over another, while a Bayes factor less than 1 suggests the opposite. Unlike frequentist p-values, the Bayes factor gives a continuous measure of evidence for one hypothesis relative to another.\n",
    "In the Bayesian approach, the hypothesis is treated probabilistically, with posterior probabilities being directly computed. This provides a more nuanced way of interpreting evidence in favor of or against a hypothesis.\n",
    "\n",
    "### 4. Interpretation of Results\n",
    "\n",
    "#### Frequentist Approach:\n",
    "\n",
    "The frequentist approach gives point estimates of parameters (such as the mean difference between groups) and provides confidence intervals that describe the range of plausible values for the parameter.\n",
    "However, the interpretation of the confidence interval is often misunderstood. It‚Äôs not a probability statement about the parameter but a statement about the method‚Äôs long-run performance.\n",
    "Hypothesis testing provides binary decisions (reject or fail to reject the null hypothesis), which can be seen as overly rigid.\n",
    "\n",
    "#### Bayesian Approach:\n",
    "\n",
    "In Bayesian ANOVA, the results are more flexible and richer. The posterior distributions give a full picture of parameter uncertainty, allowing for the calculation of credible intervals (Bayesian analogs of confidence intervals) that directly describe the probability of parameters lying within certain ranges, given the data and prior.\n",
    "Hypothesis testing in the Bayesian framework allows for more nuanced interpretation, with posterior probabilities and Bayes factors providing continuous measures of evidence. This can be more informative than the frequentist p-value, which is more of a binary decision.\n",
    "\n",
    "### 5. Model Comparison\n",
    "\n",
    "#### Frequentist Approach:\n",
    "\n",
    "In classical ANOVA, the model is fixed, and comparisons between models (such as nested models) are typically made through F-tests, which evaluate whether the inclusion of additional parameters improves the model fit.\n",
    "The goodness of fit is often assessed using the likelihood ratio or F-statistic, but these tests are limited by assumptions about the model structure and the distribution of errors.\n",
    "\n",
    "#### Bayesian Approach:\n",
    "\n",
    "In Bayesian ANOVA, model comparison is naturally handled by comparing the marginal likelihoods (also known as the model evidence) of different models. This allows for the direct comparison of different models with different numbers of parameters.\n",
    "Bayesian methods can also incorporate model uncertainty more easily, such as by using model averaging, where different models are weighted according to their posterior probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4121efc-5ff0-4876-9312-48d39b1093cf",
   "metadata": {},
   "source": [
    "## Q8. Question: You have two sets of data representing the income of two different professions\n",
    "\n",
    "Profession A: [48, 2, 55, 60, 62]\n",
    "\n",
    "Profession B: [45, 50, 55, 52, 47]\n",
    "\n",
    "Perform an F-test to determine if the variances of the two professions incomes are equal. What are your conclusions based on the F-test?\n",
    "\n",
    "Task: Use python code to calculate the F-statistic and p-value for the given data.\n",
    "\n",
    "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison.ison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40f2d7ea-8314-4c25-a5f1-6b735a1b3705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample variance for Profession A: 32.80\n",
      "Sample variance for Profession B: 15.70\n",
      "F-statistic: 2.09\n",
      "Degrees of freedom: df1 = 4, df2 = 4\n",
      "p-value: 0.4930\n",
      "Fail to reject the null hypothesis: The variances are not significantly different.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f\n",
    "\n",
    "# Data for the two professions\n",
    "profession_a = np.array([48, 52, 55, 60, 62])\n",
    "profession_b = np.array([45, 50, 55, 52, 47])\n",
    "\n",
    "# Step 1: Calculate the sample variances\n",
    "var_a = np.var(profession_a, ddof=1)  # sample variance of profession A\n",
    "var_b = np.var(profession_b, ddof=1)  # sample variance of profession B\n",
    "\n",
    "# Step 2: Calculate the F-statistic (larger variance / smaller variance)\n",
    "F_statistic = var_a / var_b if var_a > var_b else var_b / var_a\n",
    "\n",
    "# Step 3: Calculate the degrees of freedom\n",
    "df1 = len(profession_a) - 1  # degrees of freedom for profession A\n",
    "df2 = len(profession_b) - 1  # degrees of freedom for profession B\n",
    "\n",
    "# Step 4: Calculate the p-value (two-tailed test)\n",
    "# We calculate the p-value from the F-distribution's cumulative distribution function (CDF)\n",
    "p_value = 2 * min(f.cdf(F_statistic, df1, df2), 1 - f.cdf(F_statistic, df1, df2))\n",
    "\n",
    "# Output the results\n",
    "print(f\"Sample variance for Profession A: {var_a:.2f}\")\n",
    "print(f\"Sample variance for Profession B: {var_b:.2f}\")\n",
    "print(f\"F-statistic: {F_statistic:.2f}\")\n",
    "print(f\"Degrees of freedom: df1 = {df1}, df2 = {df2}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "# Conclusion based on significance level (alpha = 0.05)\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: The variances are significantly different.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: The variances are not significantly different.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281acb7-0613-4560-a16a-c897e92ee075",
   "metadata": {},
   "source": [
    "## Q9. Question: conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data\n",
    "\n",
    " Region A: [160,162,165,158,164]\n",
    "\n",
    " Region B: [172,175,170,168,174]\n",
    "\n",
    " Region C: [180,182,179,185,183]\n",
    "\n",
    " Task: Write python code to perform the one-way ANOVA and interpret the results\n",
    "\n",
    " Objective: Learn how to perform one-way ANOVA using Python and interpret F- statistic and p-value-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8923465-9880-42ec-bef0-a3fc6331722a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 67.8733\n",
      "p-value: 0.0000\n",
      "Reject the null hypothesis: There are significant differences in mean heights between the regions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Data for the three regions\n",
    "region_a = np.array([160, 162, 165, 158, 164])\n",
    "region_b = np.array([172, 175, 170, 168, 174])\n",
    "region_c = np.array([180, 182, 179, 185, 183])\n",
    "\n",
    "# Step 1: Perform the one-way ANOVA\n",
    "F_statistic, p_value = f_oneway(region_a, region_b, region_c)\n",
    "\n",
    "# Step 2: Output the F-statistic and p-value\n",
    "print(f\"F-statistic: {F_statistic:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "# Step 3: Conclusion based on significance level (alpha = 0.05)\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There are significant differences in mean heights between the regions.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There are no significant differences in mean heights between the regions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29764655-badb-4b78-ab65-011881ede593",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
